{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<img style=\"float: right; margin: 0px 0px 15px 15px;\" src=\"https://miro.medium.com/max/1400/1*QJZ6W-Pck_W7RlIDwUIN9Q.jpeg\" width=\"350px\" height=\"180px\" />\n",
    "\n",
    "```Cristopher Corona Velasco```\n",
    "\n",
    "```743940```\n",
    "\n",
    "# <font color= #8A0829> Laboratorio de Modelado de Datos </font>\n",
    "#### <font color= #2E9AFE> `Martes y Viernes (Videoconferencia) de 13:00 - 15:00 hrs`</font>\n",
    "- <Strong> Sara Eugenia Rodríguez </Strong>\n",
    "- <Strong> Año </Strong>: 2025\n",
    "- <Strong> Email: </Strong>  <font color=\"blue\"> `cd682324@iteso.mx` </font>\n",
    "___\n",
    "\n",
    "<p style=\"text-align:right;\"> Imagen recuperada de: https://miro.medium.com/max/1400/1*QJZ6W-Pck_W7RlIDwUIN9Q.jpeg</p>\n",
    "\n",
    "### <font color= #2E9AFE> Tema: XGBoost - Regresión</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperparámetros\n",
    "\n",
    "1. **eta (learning_rate)**: rango: [0,1] Es un parámetro que evita el sobreajuste. Rango ideal: 0.01 - 0.3\n",
    "2. **min_child_weight** : rango: [0,∞] En la tarea de regresión, esto corresponde al número mínimo de observaciones necesarias para cada nodo. Se utiliza para controlar el sobreajuste. Rango ideal: 1-10\n",
    "3. **max_depth**: rango: [0,∞] Profundidad máxima de un árbol. Aumentar este valor hará que el modelo sea más complejo y sea más probable que se sobreajuste. 0 indica que no hay límite de profundidad. Ten en cuenta que XGBoost consume memoria de forma agresiva al entrenar un árbol muy profundo. Se suele recomendar tomar valores entre 3 y 10.\n",
    "4. **gamma**: rango: [0,∞] método para podar \"prune\" el árbol. Rango ideal: 0-5\n",
    "5. **lambda (L2 - Ridge)**: Dado que el valor lambda está en el denominador de la similitud, a medida que la lambda aumenta, la similitud disminuirá y, por lo tanto, esto también disminuirá la ganancia. Esto permite una mayor poda, solo se conservan y sobreajustan las ramas con una puntuación de ganancia alta. Rango ideal: 0-10\n",
    "6. **subsample:** Este parámetro define la fracción de filas de datos utilizadas para construir cada árbol. Si se establece en 1.0, cada árbol utiliza los datos completos. Rango ideal: 0.6-1.0\n",
    "7. **colsample_bytree**: Muestreo de características por árbol. Rango ideal: 0.6-1.0\n",
    "8. **colsample_bylevel**: Muestreo de características por nivel del árbol. Rango ideal: 0.6-1.0\n",
    "9. **n_estimators**: Como XGBoost construye varios árboles para llegar a una predicción final, la cantidad de árboles que XGBoost puede crear está restringida por el número de rondas de boosting. Rango ideal: 100-1500\n",
    "10. **alpha (L1 - Lasso)**: Puede reducir el peso de algunas hojas a cero y, por lo tanto, ayudar a eliminar las divisiones débiles. Es especialmente útil para datos de alta dimensión para eliminar por completo las características débiles. Rango ideal: 0-1\n",
    "11. **Booster** Gbtree Gradient Boosted Decision Trees (default)\n",
    "12. **Objective** Le dice a XGBoost qué tipo de problema de predicción se está resolviendo: clasificación, regresión, clasificación, etc.\n",
    "13. **Eval_metric**:  Se utiliza para evaluar el rendimiento del modelo en los datos de validación después de cada iteración.\n",
    "14. **Eval_set**: Utilizando este parámetro, definimos los conjuntos de datos que se utilizarán para monitorear el rendimiento del modelo después de cada iteración . Este parámetro es crucial para ayudar a identificar el ajuste insuficiente o excesivo y registrar el rendimiento del modelo durante el aprendizaje.\n",
    "15. **Early_stopping_rounds** Se utiliza para evitar el sobreajuste y reducir el cálculo innecesario al detener el proceso de entrenamiento temprano si el rendimiento del modelo no mejora durante una serie de rondas.\n",
    "\n",
    "\n",
    "\n",
    "#### Algunos tips:\n",
    "- Antes de agregar la regularización, intenta controlar la complejidad mediante configuraciones de árbol más simples como max_depth, min_child_weight y gamma. Agrega lambda y alpha si ves que el modelo está sobreajustando.\n",
    "- Ajuste gamma o min_child_weight, ya que la función de ambos parámetros es podar árboles. Si su modelo no se ajusta adecuadamente a pesar de la gran profundidad, verifica si gamma y min_child_weight están configurados demasiado altos, lo que limita la capacidad del modelo para capturar patrones importantes.\n",
    "- Subsample y colsample_bytree. Si el sobreajuste sigue siendo un problema a pesar de la regularización, intenta reducir gradualmente estos valores, pero ajústalos uno por uno.\n",
    "También puedes ajustar según las características de los datos:\n",
    "    1. Conjuntos de datos con muchas variables: utiliza un colsample_bytree más bajo para reducir el ruido de variables irrelevantes.\n",
    "    2. Conjuntos de datos pequeños: utilice una submuestra más alta para evitar un ajuste insuficiente.\n",
    "    3. Grandes conjuntos de datos: ajuste ambos parámetros en combinación.\n",
    "- Usa un bajo valor de colsample_bytree cuando uses valores altos de max_depth\n",
    "- Si aumentas max_ Depth para capturar más complejidad, considera aumentar min_child_weight para contrarrestar el riesgo de sobreajuste.\n",
    "- Usa early_stopping_rounds > 0 con aalto numero de high n_estimators\n",
    "- Usa baja learning_rate con alto numero de n_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cuándo es buena idea usar XGBoost?\n",
    "- Muchos datos (filas y columnas)\n",
    "- Relaciones no lineales complejas\n",
    "- Combinación de variables numéricas y categóricas\n",
    "- Datos ruidosos o con outliers\n",
    " Necesitas alta precisión o estás en competencia (como Kaggle)\n",
    "- Buscas regularización automática ya que XGBoost incluye Lasso y Ridge\n",
    "- No necesitas tanta interpretabilidad\n",
    "\n",
    "#### Cuándo NO es buena idea usar XGBoost?\n",
    "- Muy pocos datos (ej. < 100)\n",
    "- Modelo necesita ser 100% interpretable\n",
    "- Variables 100% numéricas linealmente relacionadas\n",
    "- Trabajas con imágenes, texto, audio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "\n",
    "Veamos el famoso conjunto de datos de vivienda de California.\n",
    "\n",
    "Es un dataset que tiene relaciones no lineales, lo que lo convierte en un excelente caso de prueba.\n",
    "\n",
    "- Tamaño: más de 20.000 filas.\n",
    "- Variable objetivo: valor medio de la vivienda.\n",
    "- Desafíos: valores faltantes, relaciones no lineales y tipos de características mixtas (categóricas y continuas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "url = \"https://raw.githubusercontent.com/gakudo-ai/open-datasets/main/housing.csv\"\n",
    "df = pd.read_csv(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Observar la relacion entre las variables independientes y la dependiente\n",
    "p = sns.pairplot(df, x_vars=['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
    "       'total_bedrooms', 'population', 'households', 'median_income'], y_vars='median_house_value', height=10, aspect=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quitamos algunas variables\n",
    "#df.drop(columns=['longitude','latitude'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividimos en entrenamiento y prueba\n",
    "X = df.drop(columns=[\"median_house_value\"], errors=\"ignore\")\n",
    "y = df[\"median_house_value\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cómo sería hacer el procesamiento con una regresión lineal? \n",
    "Tenemos:\n",
    "- Valores faltantes\n",
    "- Diferentes escalas\n",
    "- Variables categoricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline para la regresion lineal\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# definimos el procesamiento para variables numericas y categoricas\n",
    "num_features = ['latitude','longitude',\n",
    "    'housing_median_age', 'total_rooms',\n",
    "    'total_bedrooms', 'population', 'households', 'median_income'\n",
    "]\n",
    "cat_features = ['ocean_proximity']\n",
    "\n",
    "# preprocesaminento numericas\n",
    "num_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')), \n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# # preprocesaminento categoricas\n",
    "cat_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', num_pipeline, num_features),\n",
    "    ('cat', cat_pipeline, cat_features)\n",
    "])\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "\n",
    "# Entrenamos el modelo\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_lr = model.predict(X_test)\n",
    "print(\"Linear Regression RMSE:\", root_mean_squared_error(y_test, y_pred_lr))\n",
    "print(\"Linear Regression R2:\", r2_score(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cómo sería hacer el procesamiento con el XGBoost?\n",
    "\n",
    "Es más sencillo ya que no tenemos que hacer escalamiento de las variables numericas ni en encoding de las variables categoricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# preprocesaminento numericas\n",
    "X_train['total_bedrooms'] = X_train['total_bedrooms'].fillna(X_train['total_bedrooms'].mean())\n",
    "X_test['total_bedrooms'] = X_test['total_bedrooms'].fillna(X_train['total_bedrooms'].mean())\n",
    "\n",
    "# # preprocesaminento categoricas\n",
    "X_train['ocean_proximity'] = X_train['ocean_proximity'].fillna(X_train['ocean_proximity'].mode()[0])\n",
    "X_test['ocean_proximity'] = X_test['ocean_proximity'].fillna(X_train['ocean_proximity'].mode()[0])\n",
    "#asegurarnos que las variables categoricas son del tipo category\n",
    "X_train['ocean_proximity'] = X_train['ocean_proximity'].astype('category')\n",
    "X_test['ocean_proximity'] = X_test['ocean_proximity'].astype('category')\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    reg_lambda=1,\n",
    "    reg_alpha=0,\n",
    "    enable_categorical=True\n",
    ")\n",
    "\n",
    "# Entrenamos el modelo\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "print(\"XGBoost RMSE:\", root_mean_squared_error(y_test, y_pred_xgb))\n",
    "print(\"XGBoost R2:\", r2_score(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin tunear tanto tenemos muy buen rendimiento en el modelo :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importancia de las variables\n",
    "xgb.plot_importance(xgb_model)\n",
    "plt.title(\"XGBoost Feature Importance\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
