{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<img style=\"float: right; margin: 0px 0px 15px 15px;\" src=\"https://www.researchgate.net/profile/Sandip-Lahiri/publication/26543790/figure/fig1/AS:310045121236994@1450931925958/A-schematic-diagram-of-the-support-vector-regression-using-e-sensitive-loss-function.png\" width=\"350px\" height=\"180px\" />\n",
    "\n",
    "\n",
    "# <font color= #8A0829> Laboratorio de Modelado de Datos </font>\n",
    "#### <font color= #2E9AFE> `Martes y Viernes (Videoconferencia) de 13:00 - 15:00 hrs`</font>\n",
    "- <Strong> Sara Eugenia Rodríguez </Strong>\n",
    "- <Strong> Año </Strong>: 2025\n",
    "- <Strong> Email: </Strong>  <font color=\"blue\"> `cd682324@iteso.mx` </font>\n",
    "___\n",
    "\n",
    "<p style=\"text-align:right;\"> Imagen recuperada de: https://www.researchgate.net/profile/Sandip-Lahiri/publication/26543790/figure/fig1/AS:310045121236994@1450931925958/A-schematic-diagram-of-the-support-vector-regression-using-e-sensitive-loss-function.png</p>\n",
    "\n",
    "### <font color= #2E9AFE> Tema:Máquinas de Soporte Vectorial basadas en Regresión (SVR)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Qué es?**\n",
    "\n",
    "- Es un algoritmo de regresión que soporta tanto problemas lineales como no lineales. \n",
    "\n",
    "- La idea es seleccionar el hiperplano regresor que mejor se ajuste a nuestro conjunto de datos de entrenamiento. \n",
    "\n",
    "- Se basa en considerar una distancia margen $\\varepsilon$, de modo que esperamos que todos los datos se encuentren en una banda o tubo entorno a nuestro hiperplano. A la hora de definir el hiperplano sólo se consideran los datos que tengan una distancia más de $\\varepsilon$ de nuestro hiperplano. En este caso esos datos serán los considerados como vectores soporte.\n",
    "\n",
    "**Diferencia regresión simple y SVR**\n",
    "\n",
    "En la regresión simple, se busca minimizar la tasa del error, mientras que en el SVR se busca ajustar el error dentro de cierto umbral, lo que significa que el trabajo del SVR es aproximar el mejor valor dentro de un margen dado llamado \"e-tubo\". \n",
    "\n",
    "<img style=\"float: center; margin: 0px 0px 15px 15px;\" src=\"https://www.mdpi.com/applsci/applsci-10-03086/article_deploy/html/images/applsci-10-03086-g004.png\" width=\"450px\" height=\"280px\" />\n",
    "\n",
    "**Algunos términos**\n",
    "\n",
    "1. Hiperplano: es la línea que ayuda a predecir la variable target\n",
    "2. Kernel: En el SVR, la regresión se realiza en una dimensión superior. Para hacer eso, necesitamos una función que debería asignar los puntos de datos a su dimensión superior. Esta función se denomina kernel. \n",
    "3. Líneas de soporte: estas son las dos líneas que se dibujan alrededor del hiperplano a una distancia de ε (épsilon). Se utiliza para crear un margen entre los puntos de datos.\n",
    "4. Vector de soporte: Es el vector que se utiliza para definir el hiperplano o podemos decir que estos son los puntos de datos extremos en el conjunto de datos que ayudan a definir el hiperplano. Estos puntos de datos se encuentran cerca del límite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**El truco del Kernel**\n",
    "\n",
    "El truco del kernel es una técnica utilizada en las máquinas de vectores soporte, para manejar problemas no lineales. En lugar de transformar explícitamente los datos a un espacio de mayor dimensionalidad, el truco del kernel permite calcular los productos escalares en ese espacio sin tener que realizar realmente la transformación. Esto permite que se pueda trabajar con relaciones no lineales entre las variables.\n",
    "\n",
    "\n",
    "**Algunos tipos de kernel**\n",
    "- Lineal\n",
    "$$K(x,x*)=x \\cdot x^{*}$$\n",
    "- Polinomial \n",
    "$$K(x,x*)=(x \\cdot x^{*} +1)^{d}$$\n",
    "- Gaussian Radial Basis \n",
    "$$K(x,x*)=\\exp^{-\\frac{\\|x-x^{*}\\|^{2}}{2\\sigma^{2}}}$$\n",
    "\n",
    "**Hiperparámetros del SVR**\n",
    "\n",
    "Los hiperparámetros son los valores que el nosotros definimos antes del entrenamiento y no se ajustan automáticamente. Estos hiperparámetros controlan aspectos como la complejidad del modelo, la regularización y la tasa de aprendizaje.\n",
    "\n",
    "- C\n",
    "- El parámetro respectivo del kernel que decidas utilizar\n",
    "- Epsilon\n",
    "\n",
    "**Cuándo usarlo?:**\n",
    "- Relaciones lineales\n",
    "- Cuando tenemos pocos datos\n",
    "- Cuando hay muchos atípicos\n",
    "- Evitar el overfitting al tener pocos datos\n",
    "- Este algoritmo funciona muy bien si se tienen los datos limpios. Si los datos están muy dispersos, no se podrá crear una fórmula adecuada.\n",
    "- Se recomienda estandarizar los datos previamente.\n",
    "  \n",
    "**Cuándo no usarlo?:**\n",
    "- No es adecuado para conjuntos de datos grandes. Lleva mucho tiempo el entrenamiento\n",
    "- Muchas variables\n",
    "- Cuando se necesita interpretabilidad\n",
    "- Datos con muchas variables categóricas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import (linear_kernel,polynomial_kernel,rbf_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% generacion de los datos\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "# #############################################################################\n",
    "# Generar datos\n",
    "X = 5 * rng.rand(100, 1)\n",
    "y = np.ravel(3*X+2)\n",
    "# Añadir ruido a las variables\n",
    "yrnd = y + 3 * (0.5 - rng.rand(X.shape[0]))\n",
    "yrnd[::20] += 50 * (0.5 - rng.rand(X.shape[0]//20))\n",
    "\n",
    "X_plot = np.linspace(0, 5, len(X))[:, None]\n",
    "\n",
    "#Graficar datos\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(X, y, c='b', label='data')\n",
    "plt.scatter(X, yrnd, c='r', s=10, label='data rnd',zorder=2)\n",
    "plt.xlabel('data')\n",
    "plt.ylabel('target')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparar con regresión lineal\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "modelo_lin = LinearRegression().fit(X,yrnd)\n",
    "Ylin_plot = modelo_lin.predict(X_plot)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(X, y, c='b', s=10, label='data')\n",
    "plt.scatter(X, yrnd, c='r', s=10, label='data rnd',zorder=2)\n",
    "plt.scatter(X_plot, Ylin_plot, c='g', s=10, label='lineal',zorder=2)\n",
    "plt.xlabel('data')\n",
    "plt.ylabel('target')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#¿Cómo se vería con el svr?\n",
    "epsilon = 0.1 #hiperparámetro\n",
    "C = 1 #hiperparametro\n",
    "model_svr = SVR(kernel='linear', epsilon=epsilon, C=C)\n",
    "model_svr.fit(X,yrnd)\n",
    "Ysvr_plot = model_svr.predict(X_plot)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(X, y, c='b', s=10, label='data')\n",
    "plt.scatter(X, yrnd, c='r', s=10, label='data rnd',zorder=2)\n",
    "plt.scatter(X_plot, Ylin_plot, c='g', s=10, label='lineal',zorder=2)\n",
    "plt.scatter(X_plot, Ysvr_plot, c='m', s=10, label='svr',zorder=2)\n",
    "plt.xlabel('data')\n",
    "plt.ylabel('target')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La línea del SVR (rosa) se acerca mucho a los valores sin ruido (azul) y tiene más inmunidad a los outliers, esto ocurre por la función de costo. Un valor grande (outlier) ya no se eleva al cuadrado, osea que sólo se afecta de forma lineal. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimensión de los datos originales\n",
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dimensión de los datos después de la transformación del kernel\n",
    "\n",
    "K_X =  linear_kernel(X)\n",
    "#K_X =  polynomial_kernel(X)\n",
    "#K_X =  rbf_kernel(X)\n",
    "\n",
    "np.shape(K_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, yrnd, test_size=0.2,\n",
    "                                                    random_state=0,\n",
    "                                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Escalamiento de los datos\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "sc_y = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVR\n",
    "epsilon = 0.1 #hiperparámetro que hay que tunear --> estos hay que modificarlos para cada problema\n",
    "C = 1 #hiperparámetro que hay que tunear --> estos hay que modificarlos para cada problema\n",
    "#Kernel Lineal\n",
    "model_svr = SVR(kernel='linear', epsilon=epsilon, C=C)\n",
    "#Kernel Radial\n",
    "epsilon= 0.1 #hiperparámetro que hay que tunear\n",
    "C = 1\n",
    "#model_svr = SVR(kernel='rbf', epsilon=epsilon, C=C)\n",
    "\n",
    "#Entreno los datos con el train\n",
    "model_svr.fit(X_train,y_train)\n",
    "\n",
    "#Predicciones contra el test\n",
    "Ypred = model_svr.predict(X_test)\n",
    "\n",
    "#R2\n",
    "print('R2 = %0.4f'%model_svr.score(X_test,y_test))\n",
    "\n",
    "#MSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print('MSE',mean_squared_error(y_test,Ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: Cómo se ven los diferentes tipos de kernel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Generar datos sintéticos no lineales\n",
    "np.random.seed(42)\n",
    "X_nuevo = np.sort(5 * np.random.rand(100, 1), axis=0)\n",
    "y_nuevo = np.sin(X).ravel() + np.random.normal(0, 0.2, X_nuevo.shape[0])\n",
    "\n",
    "# Crear modelos SVR con diferentes kernels\n",
    "svr_linear = SVR(kernel='linear', C=100, epsilon=0.1)\n",
    "svr_poly = SVR(kernel='poly', C=100, degree=3, epsilon=0.1)\n",
    "svr_rbf = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)\n",
    "\n",
    "# Ajustar los modelos\n",
    "y_pred_linear = svr_linear.fit(X_nuevo, y_nuevo).predict(X_nuevo)\n",
    "y_pred_poly = svr_poly.fit(X_nuevo, y_nuevo).predict(X_nuevo)\n",
    "y_pred_rbf = svr_rbf.fit(X_nuevo, y_nuevo).predict(X_nuevo)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Gráfico para el kernel lineal\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(X_nuevo, y_nuevo, color='darkorange', label='Datos')\n",
    "plt.plot(X_nuevo, y_pred_linear, color='navy', lw=2, label='SVR (lineal)')\n",
    "plt.title('SVR con Kernel Lineal')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "\n",
    "# Gráfico para el kernel polinómico\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(X_nuevo, y_nuevo, color='darkorange', label='Datos')\n",
    "plt.plot(X_nuevo, y_pred_poly, color='green', lw=2, label='SVR (polinómico grado 3)')\n",
    "plt.title('SVR con Kernel Polinómico')\n",
    "plt.xlabel('X')\n",
    "plt.legend()\n",
    "\n",
    "# Gráfico para el kernel RBF\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(X_nuevo, y_nuevo, color='darkorange', label='Datos')\n",
    "plt.plot(X_nuevo, y_pred_rbf, color='red', lw=2, label='SVR (RBF)')\n",
    "plt.title('SVR con Kernel RBF')\n",
    "plt.xlabel('X')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo tuneando hiperparámetros\n",
    "\n",
    "#### Métodos de tuneo de hiperparámetros + cross validation\n",
    "\n",
    "Con cross validation podemos \"tunear\" los hiperparámetros. \n",
    "\n",
    "- Decidir cuál tipo de kernel es mejor\n",
    "- Tunear el valor de C\n",
    "- Tunear los parámetros correspondientes al kernel\n",
    "\n",
    "Para hacer esto, vamos a dividir los datos de \"entrenamiento\" en entrenamiento y validación. \n",
    "- Buscamos los mejores hiperparámetros en el train\n",
    "- Observamos en la validación las métricas de performance\n",
    "- Test se guarda aparte y nunca se toca hasta el final.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import loguniform\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real\n",
    "\n",
    "n_samples = 10000\n",
    "n_features = 10\n",
    "test_size = 1000\n",
    "random_state = 42\n",
    "np.random.seed(random_state)\n",
    "\n",
    "X = np.zeros((n_samples, n_features))\n",
    "X[:, 0] = np.random.uniform(-5, 5, n_samples)\n",
    "X[:, 1] = np.random.normal(0, 3, n_samples)\n",
    "X[:, 2] = np.random.uniform(-10, 10, n_samples)\n",
    "if n_features > 3: X[:, 3] = np.random.normal(5, 2, n_samples)\n",
    "\n",
    "y = np.zeros(n_samples)\n",
    "y += 2 * X[:, 0] + 5\n",
    "y += 0.5 * (X[:, 1] ** 2) - 3 * X[:, 1]\n",
    "y += 10 * np.sin(X[:, 2] / 2)\n",
    "y += 0.7 * X[:, 0] * X[:, 3]\n",
    "noise = np.random.normal(0, 2, n_samples) * (1 + 0.1 * np.abs(X[:, 0]))\n",
    "y += noise\n",
    "\n",
    "# split into train, validation, test datasets\n",
    "X_tv, X_test, y_tv, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tv, y_tv, test_size=test_size, random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos Pipeline y CV\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svr\", SVR())\n",
    "])\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# GRID SEARCH\n",
    "param_grid = {\n",
    "    \"svr__C\": [0.1, 1, 10],\n",
    "    \"svr__epsilon\": [0.01, 0.1, 0.3],\n",
    "    \"svr__gamma\": [0.01, 0.1, 1],\n",
    "    \"svr__kernel\": [\"rbf\"]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid=param_grid, cv=cv,\n",
    "                           scoring=\"neg_mean_squared_error\", n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Mejores parametros (Grid):\", grid_search.best_params_)\n",
    "print(\"MSE (Grid - Val):\", mean_squared_error(y_val, grid_search.predict(X_val)))\n",
    "\n",
    "# RANDOM SEARCH\n",
    "\n",
    "param_dist = {\n",
    "    \"svr__C\": loguniform(1e-3, 10),\n",
    "    \"svr__epsilon\": loguniform(1e-3, 0.3),\n",
    "    \"svr__gamma\": loguniform(1e-3, 1),\n",
    "    \"svr__kernel\": [\"rbf\"]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(pipe, param_distributions=param_dist,\n",
    "                                   n_iter=30, cv=cv,\n",
    "                                   scoring=\"neg_mean_squared_error\",\n",
    "                                   random_state=42, n_jobs=-1)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nMejores parametros (Random):\", random_search.best_params_)\n",
    "print(\"MSE (Random - Val):\", mean_squared_error(y_val, random_search.predict(X_val)))\n",
    "\n",
    "\n",
    "# BAYESIAN OPTIMIZATION\n",
    "\n",
    "search_spaces = {\n",
    "    \"svr__C\": Real(1e-3, 10, prior=\"log-uniform\"),\n",
    "    \"svr__epsilon\": Real(1e-3, 0.3, prior=\"log-uniform\"),\n",
    "    \"svr__gamma\": Real(1e-3, 1.0, prior=\"log-uniform\"),\n",
    "    \"svr__kernel\": [\"rbf\"]\n",
    "}\n",
    "\n",
    "bayes_search = BayesSearchCV(pipe, search_spaces=search_spaces,\n",
    "                             n_iter=30, cv=cv,\n",
    "                             scoring=\"neg_mean_squared_error\",\n",
    "                             random_state=42, n_jobs=-1)\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nMejores parametros (Bayes):\", bayes_search.best_params_)\n",
    "print(\"MSE (Bayes - Val):\", mean_squared_error(y_val, bayes_search.predict(X_val)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pasos después del tuneo de hiperparámetros**\n",
    "\n",
    "1. Obtener los mejores hiperparámetros\n",
    "2. Volver a entrenar un nuevo modelo con los mejores hiperparámetros\n",
    "    - Crear un nuevo SVR con esos hiperparámetros\n",
    "    - Entrenar ese nuevo modelo con todo el conjunto de datos de entrenamiento, que incluye tanto X_train como X_val\n",
    "3. Evaluar en el conjunto de prueba (X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación final en TEST\n",
    "\n",
    "# Tomamos el mejor de los tres (bayesiano)\n",
    "best_model = bayes_search.best_estimator_\n",
    "\n",
    "# Reentrenamos con todo train+val\n",
    "best_model.fit(X_tv, y_tv)\n",
    "\n",
    "# Evaluamos en test\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "print(\"\\nFinal MSE on test set (Bayes best):\", mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cómo sé si está sobreajustand?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medimos en el train, validation y test\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_pred_train = best_model.predict(X_train)\n",
    "print(\"MSE (train):\", mean_squared_error(y_train, y_pred_train))\n",
    "\n",
    "y_pred_val = best_model.predict(X_val)\n",
    "print(\"MSE (val):\", mean_squared_error(y_val, y_pred_val))\n",
    "\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "print(\"MSE (test):\", mean_squared_error(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Curva de aprendizaje\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_model, X_tv, y_tv,\n",
    "    cv=5, scoring=\"neg_mean_squared_error\",\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5), n_jobs=-1\n",
    ")\n",
    "\n",
    "train_mse = -train_scores.mean(axis=1)\n",
    "val_mse = -val_scores.mean(axis=1)\n",
    "\n",
    "plt.plot(train_sizes, train_mse, label=\"Train MSE\")\n",
    "plt.plot(train_sizes, val_mse, label=\"Validation MSE\")\n",
    "plt.xlabel(\"Training size\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
